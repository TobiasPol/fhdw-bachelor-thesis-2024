@article{AccuracyPrecision2024,
  title = {Accuracy and Precision},
  year = {2024},
  month = apr,
  journal = {Wikipedia},
  urldate = {2024-05-31},
  abstract = {Accuracy and precision are two measures of observational error. Accuracy is how close a given set of measurements (observations or readings) are to their true value. Precision is how close the measurements are to each other. In other words: Precision is a description of random errors (a measure of statistical variability). Accuracy has two definitions, per ISO: More commonly, a description of systematic errors (a measure of statistical bias of a given measure of central tendency). Low accuracy causes a difference between a result and a true value. This secondary measure is referred to as trueness by ISO. A combination of both types of observational error (random and systematic), so high accuracy requires both high precision and high trueness. In the first, more common definition of "accuracy" above, the concept is independent of "precision", so a particular set of data can be said to be accurate, precise, both, or neither. In simpler terms, given a statistical sample or set of data points from repeated measurements of the same quantity, the sample or set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if their standard deviation is relatively small.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1220739681}
}

@article{asgariContinuousDistributedRepresentation2015,
  title = {Continuous {{Distributed Representation}} of {{Biological Sequences}} for {{Deep Proteomics}} and {{Genomics}}},
  author = {Asgari, Ehsaneddin and Mofrad, M. R.},
  year = {2015},
  journal = {PLoS ONE},
  volume = {10},
  doi = {10.1371/journal.pone.0141287},
  urldate = {2024-06-12},
  abstract = {We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93\%{\textpm}0.06\% is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8\% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0\% accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. Importantly, this model needs to be trained only once and can then be applied to extract a comprehensive set of information regarding proteins of interest. Moreover, this representation can be considered as pre-training for various applications of deep learning in bioinformatics. The related data is available at Life Language Processing Website: http://llp.berkeley.edu and Harvard Dataverse: http://dx.doi.org/10.7910/DVN/JMFHTN.},
  file = {/Users/tobias.polley/Zotero/storage/LTXIAIKV/Asgari and Mofrad - 2015 - Continuous Distributed Representation of Biologica.pdf;/Users/tobias.polley/Zotero/storage/YC8UJZIV/eac69514cfcd51adb76131dfce4112d3.html}
}

@article{belloTheoreticalApproachMechanism2000,
  title = {A Theoretical Approach to the Mechanism of Biological Oxidation of Organophosphorus Pesticides},
  author = {Bello, Angelica and Carreon, Yessica and {Nava-Ocampo}, Alejandro},
  year = {2000},
  month = aug,
  journal = {Toxicology},
  volume = {149},
  pages = {63--8},
  doi = {10.1016/S0300-483X(00)00222-5},
  abstract = {Organophosphorus pesticides are the most common classes involved in poisonings related to pesticides. We used enzymatic activity of chloroperoxidase on the metabolism of some phosphorothioate pesticides published previously and molecular mechanics methods to perform a theoretical approach of the mechanism of biological oxidation of this class of pesticides. The molecular structure of eight pesticides were optimized by molecular mechanics methods using the CAChe program package for biomolecules, ver. 3.11 (Oxford Molecular Ltd., Campbell, CA). Total energy resulted from the structure optimization process and the partial charges of both phosphorus and sulfur were computed for every pesticide. Phosphorus partial charge and enzymatic activity were significantly related by linear regression analysis (r=0.82, P{$<$}0.05). Analyzing our results and using previously reported enzymatic activity of chloroperoxidase on these pesticides, we deduced chemical events involved in activation of the active site of chloroperoxidase and proposed a novel mechanism of oxidation for this class of pesticides. This mechanism will also help to understand the oxidation process of pesticides by cytochrome P450, and production of toxic metabolites.}
}

@article{bergstraRandomSearchHyperParameter,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent ``High Throughput'' methods achieve surprising success---they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  langid = {english},
  keywords = {No DOI found},
  file = {/Users/tobias.polley/Zotero/storage/62DMYMT4/Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf}
}

@article{bergstraRandomSearchHyperParameter2012,
  title = {Random {{Search}} for {{Hyper-Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Y.},
  year = {2012},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {13},
  pages = {281--305},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  keywords = {No DOI found}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {006.4},
  file = {/Users/tobias.polley/Zotero/storage/XGNL3V7F/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  urldate = {2024-06-12},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {/Users/tobias.polley/Zotero/storage/GRRLVMKM/Breiman - 2001 - Random Forests.pdf}
}

@article{chiaFunctionMicrobialEnzymes2024,
  title = {The Function of Microbial Enzymes in Breaking down Soil Contaminated with Pesticides: A Review},
  shorttitle = {The Function of Microbial Enzymes in Breaking down Soil Contaminated with Pesticides},
  author = {Chia, Xing Kai and Hadibarata, Tony and Kristanti, Risky Ayu and Jusoh, Muhammad Noor Hazwan and Tan, Inn Shi and Foo, Henry Chee Yew},
  year = {2024},
  month = may,
  journal = {Bioprocess and Biosystems Engineering},
  volume = {47},
  number = {5},
  pages = {597--620},
  issn = {1615-7605},
  doi = {10.1007/s00449-024-02978-6},
  urldate = {2024-06-06},
  abstract = {The use of pesticides and the subsequent accumulation of residues in the soil has become a worldwide problem. Organochlorine (OC) pesticides have spread widely in the environment and caused contamination from past agricultural activities. This article reviews the bioremediation of pesticide compounds in soil using microbial enzymes, including the enzymatic degradation pathway and the recent development of enzyme-mediated bioremediation. Enzyme-mediated bioremediation is divided into phase I and phase II, where the former increases the solubility of pesticide compounds through oxidation--reduction and hydrolysis reactions, while the latter transforms toxic pollutants into less toxic or nontoxic products through conjugation reactions. The identified enzymes that can degrade OC insecticides include dehalogenases, phenol hydroxylase, and laccases. Recent developments to improve enzyme-mediated bioremediation include immobilization, encapsulation, and protein engineering, which ensure its stability, recyclability, handling and storage, and better control of the reaction.},
  langid = {english},
  keywords = {Bioremediation,Contaminated soil,Microbial enzyme,Organochlorine,Pesticide},
  file = {/Users/tobias.polley/Zotero/storage/U5G49UTW/Chia et al. - 2024 - The function of microbial enzymes in breaking down.pdf}
}

@misc{ConsistencyRandomForests,
  title = {Consistency of {{Random Forests}} - {{Consensus}}},
  urldate = {2024-06-12},
  howpublished = {https://consensus.app/papers/consistency-random-forests-scornet/424f3833f7e65629a21b8aae4940dcbc/?utm\_source=chatgpt},
  file = {/Users/tobias.polley/Zotero/storage/CU7F3GIG/424f3833f7e65629a21b8aae4940dcbc.html}
}

@misc{DeEPnDeepNeural,
  title = {{{DeEPn}}: A Deep Neural Network Based Tool for Enzyme Functional Annotation - {{Consensus}}},
  urldate = {2024-05-29},
  howpublished = {https://consensus.app/papers/deepn-network-based-tool-annotation-semwal/c62a6c023f2d5b2f9348b36f6329daae/?utm\_source=chatgpt}
}

@article{diaz-uriarteGeneSelectionClassification2006,
  title = {Gene Selection and Classification of Microarray Data Using Random Forest},
  author = {{D{\'i}az-Uriarte}, R. and Andr{\'e}s, S. {\'A} D.},
  year = {2006},
  journal = {BMC Bioinformatics},
  volume = {7},
  pages = {3--3},
  doi = {10.1186/1471-2105-7-3},
  urldate = {2024-06-12},
  abstract = {BackgroundSelection of relevant genes for sample classification is a common task in most gene expression studies, where researchers try to identify the smallest possible set of genes that can still achieve good predictive performance (for instance, for future use with diagnostic purposes in clinical practice). Many gene selection approaches use univariate (gene-by-gene) rankings of gene relevance and arbitrary thresholds to select the number of genes, can only be applied to two-class problems, and use gene selection ranking criteria unrelated to the classification algorithm. In contrast, random forest is a classification algorithm well suited for microarray data: it shows excellent performance even when most predictive variables are noise, can be used when the number of variables is much larger than the number of observations and in problems involving more than two classes, and returns measures of variable importance. Thus, it is important to understand the performance of random forest with microarray data and its possible use for gene selection.ResultsWe investigate the use of random forest for classification of microarray data (including multi-class problems) and propose a new method of gene selection in classification problems based on random forest. Using simulated and nine microarray data sets we show that random forest has comparable performance to other classification methods, including DLDA, KNN, and SVM, and that the new gene selection procedure yields very small sets of genes (often smaller than alternative methods) while preserving predictive accuracy.ConclusionBecause of its performance and features, random forest and gene selection using random forest should probably become part of the "standard tool-box" of methods for class prediction and gene selection with microarray data.},
  file = {/Users/tobias.polley/Zotero/storage/WVE47CS4/Díaz-Uriarte and Andrés - 2006 - Gene selection and classification of microarray da.pdf;/Users/tobias.polley/Zotero/storage/NTD7RSYF/19afa230099b541c97bfd625225729b1.html}
}

@article{emmert-streibIntroductoryReviewDeep2020,
  title = {An {{Introductory Review}} of {{Deep Learning}} for {{Prediction Models With Big Data}}},
  author = {{Emmert-Streib}, F. and Yang, Zhenyi and Feng, Han and Tripathi, S. and Dehmer, M.},
  year = {2020},
  journal = {Frontiers in Artificial Intelligence},
  volume = {3},
  doi = {10.3389/frai.2020.00004},
  urldate = {2024-05-31},
  abstract = {Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly---in an almost Lego-like manner---to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.},
  file = {/Users/tobias.polley/Zotero/storage/5856BVP6/Emmert-Streib et al. - 2020 - An Introductory Review of Deep Learning for Predic.pdf}
}

@article{Fscore2024,
  title = {F-Score},
  year = {2024},
  month = may,
  journal = {Wikipedia},
  urldate = {2024-05-31},
  abstract = {In statistical analysis of binary classification and information retrieval systems, the F-score or F-measure is a measure of predictive performance. It is calculated from the precision and recall of the test, where the precision is the number of true positive results divided by the number of all samples predicted to be positive, including those not identified correctly, and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive. Precision is also known as positive predictive value, and recall is also known as sensitivity in diagnostic binary classification. The F1 score is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric. The more generic                                    F                        {$\beta$}                                     \{{\textbackslash}displaystyle F\_\{{\textbackslash}beta \}\}     score applies additional weights, valuing one of precision or recall more than the other. The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if precision and recall are zero.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1225870376}
}

@article{fuDegradationPesticidesDiazinon2021,
  title = {Degradation of Pesticides Diazinon and Diazoxon by Phosphotriesterase: Insight into Divergent Mechanisms from {{QM}}/{{MM}} and {{MD}} Simulations.},
  shorttitle = {Degradation of Pesticides Diazinon and Diazoxon by Phosphotriesterase},
  author = {Fu, Yuzhuang and Zhang, Yuwei and Fan, Fangfang and Wang, Binju and Cao, Z.},
  year = {2021},
  journal = {Physical chemistry chemical physics : PCCP},
  doi = {10.1039/d1cp05034f},
  urldate = {2024-05-29},
  abstract = {Enzymatic hydrolysis by phosphotriesterase (PTE) is one of the most effective ways of degrading organophosphorus pesticides, but the catalytic efficiency depends on the structural features of substrates. Here the enzymatic degradation of diazinon (DIN) and diazoxon (DON), characterized by PS and PO, respectively, have been investigated by QM/MM calculations and MM MD simulations. Our calculations demonstrate that the hydrolysis of DON (with PO) is inevitably initiated by the nucleophilic attack of the bridging-OH- on the phosphorus center, while for DIN (with PS), we proposed a new degradation mechanism, initiated by the nucleophilic attack of the Zn{$\alpha$}-bound water molecule, for its low-energy pathway. For both DIN and DON, the hydrolytic reaction is predicted to be the rate-limiting step, with energy barriers of 18.5 and 17.7 kcal mol-1, respectively. The transportation of substrates to the active site, the release of the leaving group and the degraded product are generally verified to be favorable by MD simulations via umbrella sampling, both thermodynamically and dynamically. The side-chain residues Phe132, Leu271 and Tyr309 play the gate-switching role to manipulate substrate delivery and product release. In comparison with the DON-enzyme system, the degraded product of DIN is more easily released from the active site. These new findings will contribute to the comprehensive understanding of the enzymatic degradation of toxic organophosphorus compounds by PTE.}
}

@article{gainzaDecipheringInteractionFingerprints2020,
  title = {Deciphering Interaction Fingerprints from Protein Molecular Surfaces Using Geometric Deep Learning},
  author = {Gainza, P. and Sverrisson, F. and Monti, F. and Rodol{\`a}, E. and Boscaini, D. and Bronstein, M. M. and Correia, B. E.},
  year = {2020},
  month = feb,
  journal = {Nature Methods},
  volume = {17},
  number = {2},
  pages = {184--192},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0666-6},
  urldate = {2024-06-22},
  abstract = {Predicting interactions between proteins and other biomolecules solely based on structure remains a challenge in biology. A high-level representation of protein structure, the molecular surface, displays patterns of chemical and geometric features that fingerprint a protein's modes of interactions with other biomolecules. We hypothesize that proteins participating in similar interactions may share common fingerprints, independent of their evolutionary history. Fingerprints may be difficult to grasp by visual analysis but could be learned from large-scale datasets. We present MaSIF (molecular surface interaction fingerprinting), a conceptual framework based on a geometric deep learning method to capture fingerprints that are important for specific biomolecular interactions. We showcase MaSIF with three prediction challenges: protein pocket-ligand prediction, protein--protein interaction site prediction and ultrafast scanning of protein surfaces for prediction of protein--protein complexes. We anticipate that our conceptual framework will lead to improvements in our understanding of protein function and design.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Machine learning,Protein function predictions,Protein structure predictions,Proteins}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2024-06-20},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{huExploringEvolutionbasedFree2022,
  title = {Exploring Evolution-Based \& -Free Protein Language Models as Protein Function Predictors},
  author = {Hu, Mingyang and Yuan, Fajie and Yang, Kevin K. and Ju, Fusong and Su, Jin and Wang, Hui and Yang, Fei and Ding, Qiuyang},
  year = {2022},
  journal = {ArXiv},
  volume = {abs/2206.06583},
  doi = {10.48550/arXiv.2206.06583},
  urldate = {2024-06-22},
  abstract = {Large-scale Protein Language Models (PLMs) have improved performance in protein prediction tasks, ranging from 3D structure prediction to various function predictions. In particular, AlphaFold [20], a ground-breaking AI system, could potentially reshape structural biology. However, the utility of the PLM module in AlphaFold, Evoformer, has not been explored beyond structure prediction. In this paper, we investigate the representation ability of three popular PLMs: ESM-1b (single sequence) [32], MSA-Transformer (multiple sequence alignment) [27] and Evoformer (structural), with a special focus on Evoformer. Specifically, we aim to answer the following key questions: (i) Does the Evoformer trained as part of AlphaFold produce representations amenable to predicting protein function? (ii) If yes, can Evoformer replace ESM-1b and MSA-Transformer? (iii) How much do these PLMs rely on evolution-related protein data? In this regard, are they complementary to each other? We compare these models by empirical study along with new insights and conclusions. Finally, we release code and datasets for reproducibility.}
}

@article{krivakP2RankMachineLearning2018,
  title = {{{P2Rank}}: Machine Learning Based Tool for Rapid and Accurate Prediction of Ligand Binding Sites from Protein Structure},
  shorttitle = {{{P2Rank}}},
  author = {Kriv{\'a}k, Radoslav and Hoksza, David},
  year = {2018},
  month = aug,
  journal = {Journal of Cheminformatics},
  volume = {10},
  number = {1},
  pages = {39},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0285-8},
  urldate = {2024-05-29},
  abstract = {Ligand binding site prediction from protein structure has many applications related to elucidation of protein function and structure based drug discovery. It often represents only one step of many in complex computational drug design efforts. Although many methods have been published to date, only few of them are suitable for use in automated pipelines or for processing large datasets. These use cases require stability and speed, which disqualifies many of the recently introduced tools that are either template based or available only as web servers.},
  keywords = {Binding site prediction,Ligand binding sites,Machine learning,Protein pockets,Protein surface descriptors,Random forests},
  file = {/Users/tobias.polley/Zotero/storage/DPUM26V8/Krivák and Hoksza - 2018 - P2Rank machine learning based tool for rapid and .pdf;/Users/tobias.polley/Zotero/storage/VX3GIXPD/s13321-018-0285-8.html}
}

@article{krstajicCrossvalidationPitfallsWhen2014,
  title = {Cross-Validation Pitfalls When Selecting and Assessing Regression and Classification Models},
  author = {Krstajic, D. and Buturovic, L. and Leahy, D. and Thomas, Simon},
  year = {2014},
  journal = {Journal of Cheminformatics},
  volume = {6},
  doi = {10.1186/1758-2946-6-10},
  urldate = {2024-06-25},
  abstract = {BackgroundWe address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches.MethodsWe describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case.ResultsWe show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models.ConclusionsWe demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error.},
  file = {/Users/tobias.polley/Zotero/storage/E93A8Y8X/Krstajic et al. - 2014 - Cross-validation pitfalls when selecting and asses.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2024-05-16},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Mathematics and computing},
  file = {/Users/tobias.polley/Zotero/storage/JELDQMA8/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{liDEEPreSequencebasedEnzyme2017,
  title = {{{DEEPre}}: Sequence-Based Enzyme {{EC}} Number Prediction by Deep Learning},
  shorttitle = {{{DEEPre}}},
  author = {Li, Yu and Wang, Sheng and Umarov, Ramzan and Xie, Bingqing and Fan, M. and Li, Lihua and Gao, Xin},
  year = {2017},
  journal = {Bioinformatics},
  volume = {34},
  pages = {760--769},
  doi = {10.1093/bioinformatics/btx680},
  urldate = {2024-05-29},
  abstract = {Motivation Annotation of enzyme function has a broad range of applications, such as metagenomics, industrial biotechnology, and diagnosis of enzyme deficiency-caused diseases. However, the time and resource required make it prohibitively expensive to experimentally determine the function of every enzyme. Therefore, computational enzyme function prediction has become increasingly important. In this paper, we develop such an approach, determining the enzyme function by predicting the Enzyme Commission number. Results We propose an end-to-end feature selection and classification model training approach, as well as an automatic and robust feature dimensionality uniformization method, DEEPre, in the field of enzyme function prediction. Instead of extracting manually crafted features from enzyme sequences, our model takes the raw sequence encoding as inputs, extracting convolutional and sequential features from the raw encoding based on the classification result to directly improve the prediction performance. The thorough cross-fold validation experiments conducted on two large-scale datasets show that DEEPre improves the prediction performance over the previous state-of-the-art methods. In addition, our server outperforms five other servers in determining the main class of enzymes on a separate low-homology dataset. Two case studies demonstrate DEEPre's ability to capture the functional difference of enzyme isoforms. Availability and implementation The server could be accessed freely at http://www.cbrc.kaust.edu.sa/DEEPre. Contact xin.gao@kaust.edu.sa Supplementary information Supplementary data are available at Bioinformatics online.},
  file = {/Users/tobias.polley/Zotero/storage/DUKBWBT8/Li et al. - 2017 - DEEPre sequence-based enzyme EC number prediction.pdf}
}

@article{liuAttentionMechanismEnhanced2019,
  title = {Attention Mechanism Enhanced {{LSTM}} with Residual Architecture and Its Application for Protein-Protein Interaction Residue Pairs Prediction},
  author = {Liu, Jiale and Gong, Xinqi},
  year = {2019},
  journal = {BMC Bioinformatics},
  volume = {20},
  doi = {10.1186/s12859-019-3199-1},
  urldate = {2024-06-23},
  abstract = {BackgroundRecurrent neural network(RNN) is a good way to process sequential data, but the capability of RNN to compute long sequence data is inefficient. As a variant of RNN, long short term memory(LSTM) solved the problem in some extent. Here we improved LSTM for big data application in protein-protein interaction interface residue pairs prediction based on the following two reasons. On the one hand, there are some deficiencies in LSTM, such as shallow layers, gradient explosion or vanishing, etc. With a dramatic data increasing, the imbalance between algorithm innovation and big data processing has been more serious and urgent. On the other hand, protein-protein interaction interface residue pairs prediction is an important problem in biology, but the low prediction accuracy compels us to propose new computational methods.ResultsIn order to surmount aforementioned problems of LSTM, we adopt the residual architecture and add attention mechanism to LSTM. In detail, we redefine the block, and add a connection from front to back in every two layers and attention mechanism to strengthen the capability of mining information. Then we use it to predict protein-protein interaction interface residue pairs, and acquire a quite good accuracy over 72\%. What's more, we compare our method with random experiments, PPiPP, standard LSTM, and some other machine learning methods. Our method shows better performance than the methods mentioned above.ConclusionWe present an attention mechanism enhanced LSTM with residual architecture, and make deeper network without gradient vanishing or explosion to a certain extent. Then we apply it to a significant problem-- protein-protein interaction interface residue pairs prediction and obtain a better accuracy than other methods. Our method provides a new approach for protein-protein interaction computation, which will be helpful for related biomedical researches.},
  file = {/Users/tobias.polley/Zotero/storage/5H5K6XVF/Liu and Gong - 2019 - Attention mechanism enhanced LSTM with residual ar.pdf}
}

@article{munneckeEnzymaticHydrolysisOrganophosphate1976a,
  title = {Enzymatic Hydrolysis of Organophosphate Insecticides, a Possible Pesticide Disposal Method},
  author = {Munnecke, D.},
  year = {1976},
  journal = {Applied and Environmental Microbiology},
  volume = {32},
  pages = {7--13},
  doi = {10.1128/aem.32.1.7-13.1976},
  urldate = {2024-05-30},
  abstract = {A crude cell extract from a mixed bacterial culture growing on parathion, an organophosphate insecticide, hydrolyzed parathion (21 C) at a rate of 416 nmol/min per mg of protein. This rate of enzymatic hydrolysis, when compared with chemical hydrolysis by 0.1 N sodium hydroxide at 40 C, was 2, 450 times faster. Eight of 12 commonly used organophosphate insecticides were enzymatically hydrolyzed with this enzyme preparation at rates ranging from 12 to 1,360 nmol/min per mg of protein. Seven pesticides were hydrolyzed at rates significantly higher (40 to 1,005 times faster) than chemical hydrolysis. The pH optimum for enzymatic hydrolysis of the eight pesticides ranged from 8.5 to 9.5, with less than 50\% of maximal activity expressed at pH 7.0. Maximal enzyme activity occurred at 35 C. The crude extract lost its activity at the rate of only 0.75\%/day when stored at 6 C. Eight organic solvents, ranging from methanol to hexane, at low concentrations stimulated enzymatic hydrolysis by 3 to 20\%, whereas at higher concentrations (1,000 mg/liter) they inhibited the reaction (9 to 50\%). Parathion metabolites p-nitrophenol, hydroquinone, and diethylthiophosphoric acid, at up to 100-mg/liter concentrations, did not significantly influence enzyme activity.},
  file = {/Users/tobias.polley/Zotero/storage/X4NSVEW5/Munnecke - 1976 - Enzymatic hydrolysis of organophosphate insecticid.pdf}
}

@article{murugesanDeepCompareVisualInteractive2019,
  title = {{{DeepCompare}}: {{Visual}} and {{Interactive Comparison}} of {{Deep Learning Model Performance}}},
  shorttitle = {{{DeepCompare}}},
  author = {Murugesan, Sugeerth and Malik, Sana and Du, F. and Koh, Eunyee and Lai, T.},
  year = {2019},
  journal = {IEEE Computer Graphics and Applications},
  volume = {39},
  pages = {47--59},
  doi = {10.1109/MCG.2019.2919033},
  urldate = {2024-05-31},
  abstract = {Deep learning models have become the state-of-the-art for many tasks, from text sentiment analysis to facial image recognition. However, understanding why certain models perform better than others or how one model learns differently than another is often difficult yet critical for increasing their effectiveness, improving prediction accuracy, and enabling fairness. Traditional methods for comparing models' efficacy, such as accuracy, precision, and recall provide a quantitative view of performance; however, the qualitative intricacies of why one model performs better than another are hidden. In this paper, we interview machine learning practitioners to understand their evaluation and comparison workflow. From there, we iteratively design a visual analytic approach, DeepCompare, to systematically compare the results of deep learning models, in order to provide insight into the model behavior and interactively assess tradeoffs between two such models. The tool allows users to evaluate model results, identify and compare activation patterns for misclassifications, and link the test results back to specific neurons. We conduct a preliminary evaluation through two real-world case studies to show that experts can make more informed decisions about the effectiveness of different types of models, understand in more detail the strengths and weaknesses of the models, and holistically evaluate the behavior of the models.}
}

@misc{polleyTobiasPolDeepZyme2024,
  title = {{{TobiasPol}}/{{DeepZyme}}},
  author = {Polley, Tobias},
  year = {2024},
  month = may,
  urldate = {2024-06-05},
  abstract = {Combining the tools of Natural Language Processing and Multi Task Networks to predict the Enzyme Commision number.},
  copyright = {MIT}
}

@phdthesis{poshyvailo-strubeModellingSimulationsEnzymecatalyzed2015a,
  title = {Modelling and Simulations of Enzyme-Catalyzed Reactions},
  author = {{Poshyvailo-Strube}, Liubov},
  year = {2015},
  month = jun,
  doi = {10.13140/RG.2.2.30420.63369},
  file = {/Users/tobias.polley/Zotero/storage/22JEFDP9/Poshyvailo-Strube - 2015 - Modelling and simulations of enzyme-catalyzed reac.pdf}
}

@article{PrecisionRecall2024,
  title = {Precision and Recall},
  year = {2024},
  month = apr,
  journal = {Wikipedia},
  urldate = {2024-05-31},
  abstract = {In pattern recognition, information retrieval, object detection and classification (machine learning), precision and recall are performance metrics that apply to data retrieved from a collection, corpus or sample space. Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances. Written as a formula: Recall (also known as sensitivity) is the fraction of relevant instances that were retrieved. Written as a formula: Both precision and recall are therefore based on relevance.  Consider a computer program for recognizing dogs (the relevant element) in a digital photograph. Upon processing a picture which contains ten cats and twelve dogs, the program identifies eight dogs. Of the eight elements identified as dogs, only five actually are dogs (true positives), while the other three are cats (false positives). Seven dogs were missed (false negatives), and seven cats were correctly excluded (true negatives). The program's precision is then 5/8 (true positives / selected elements) while its recall is 5/12 (true positives / relevant elements). Adopting a hypothesis-testing approach from statistics, in which, in this case, the null hypothesis is that a given item is irrelevant (i.e., not a dog), absence of type I and type II errors (i.e., perfect specificity and sensitivity of 100\% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative).   More generally, recall is simply the complement of the type II error rate (i.e., one minus the type II error rate). Precision is related to the type I error rate, but in a slightly more complicated way, as it also depends upon the prior distribution of seeing a relevant vs. an irrelevant item. The above cat and dog example contained 8 - 5 = 3 type I errors (false positives) out of 10 total cats (true negatives), for a type I error rate of 3/10, and 12 - 5 = 7 type II errors (false negatives), for a type II error rate of 7/12.  Precision can be seen as a measure of quality, and recall as a measure of quantity.  Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1220938692}
}

@article{ReceiverOperatingCharacteristic2024,
  title = {Receiver Operating Characteristic},
  year = {2024},
  month = may,
  journal = {Wikipedia},
  urldate = {2024-05-31},
  abstract = {A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the performance of a binary classifier model (can be used for multi class classification as well) at varying threshold values. The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting. The ROC can also be thought of as a plot of the statistical power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of false positive rate. Given the probability distributions for both true positive and false positive are known, the ROC curve is obtained as the cumulative distribution function (CDF, area under the probability distribution from                         -         {$\infty$}                 \{{\textbackslash}displaystyle -{\textbackslash}infty \}     to the discrimination threshold) of the detection probability in the y-axis versus the CDF of the false positive probability on the x-axis. ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1226127318}
}

@article{rivesBiologicalStructureFunction2021,
  title = {Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},
  author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
  year = {2021},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {15},
  pages = {e2016239118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2016239118},
  urldate = {2024-06-24},
  abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
  file = {/Users/tobias.polley/Zotero/storage/DMZFXAFS/Rives et al. - 2021 - Biological structure and function emerge from scal.pdf}
}

@article{robinsonEnzymesPrinciplesBiotechnological2015,
  title = {Enzymes: Principles and Biotechnological Applications},
  shorttitle = {Enzymes},
  author = {Robinson, Peter K.},
  year = {2015},
  month = nov,
  journal = {Essays in Biochemistry},
  volume = {59},
  pages = {1--41},
  issn = {0071-1365},
  doi = {10.1042/bse0590001},
  urldate = {2024-06-06},
  abstract = {Enzymes are biological catalysts (also known as biocatalysts) that speed up biochemical reactions in living organisms, and which can be extracted from cells and then used to catalyse a wide range of commercially important processes. This chapter covers the basic principles of enzymology, such as classification, structure, kinetics and inhibition, and also provides an overview of industrial applications. In addition, techniques for the purification of enzymes are discussed.},
  pmcid = {PMC4692135},
  pmid = {26504249},
  file = {/Users/tobias.polley/Zotero/storage/S9SXG6LT/Robinson - 2015 - Enzymes principles and biotechnological applicati.pdf}
}

@article{singhMicrobialDegradationOrganophosphorus2006,
  title = {Microbial Degradation of Organophosphorus Compounds},
  author = {Singh, Brajesh K. and Walker, Allan},
  year = {2006},
  month = may,
  journal = {FEMS Microbiology Reviews},
  volume = {30},
  number = {3},
  pages = {428--471},
  issn = {0168-6445},
  doi = {10.1111/j.1574-6976.2006.00018.x},
  urldate = {2024-06-06},
  abstract = {Synthetic organophosphorus compounds are used as pesticides, plasticizers, air fuel ingredients and chemical warfare agents. Organophosphorus compounds are the most widely used insecticides, accounting for an estimated 34\% of world-wide insecticide sales. Contamination of soil from pesticides as a result of their bulk handling at the farmyard or following application in the field or accidental release may lead occasionally to contamination of surface and ground water. Several reports suggest that a wide range of water and terrestrial ecosystems may be contaminated with organophosphorus compounds. These compounds possess high mammalian toxicity and it is therefore essential to remove them from the environments. In addition, about 200 000 metric tons of nerve (chemical warfare) agents have to be destroyed world-wide under Chemical Weapons Convention (1993). Bioremediation can offer an efficient and cheap option for decontamination of polluted ecosystems and destruction of nerve agents. The first micro-organism that could degrade organophosphorus compounds was isolated in 1973 and identified as Flavobacterium sp. Since then several bacterial and a few fungal species have been isolated which can degrade a wide range of organophosphorus compounds in liquid cultures and soil systems. The biochemistry of organophosphorus compound degradation by most of the bacteria seems to be identical, in which a structurally similar enzyme called organophosphate hydrolase or phosphotriesterase catalyzes the first step of the degradation. organophosphate hydrolase encoding gene opd (organophosphate degrading) gene has been isolated from geographically different regions and taxonomically different species. This gene has been sequenced, cloned in different organisms, and altered for better activity and stability. Recently, genes with similar function but different sequences have also been isolated and characterized. Engineered microorganisms have been tested for their ability to degrade different organophosphorus pollutants, including nerve agents. In this article, we review and propose pathways for degradation of some organophosphorus compounds by microorganisms. Isolation, characterization, utilization and manipulation of the major detoxifying enzymes and the molecular basis of degradation are discussed. The major achievements and technological advancements towards bioremediation of organophosphorus compounds, limitations of available technologies and future challenge are also discussed.},
  file = {/Users/tobias.polley/Zotero/storage/FXBHQJRY/Singh and Walker - 2006 - Microbial degradation of organophosphorus compound.pdf;/Users/tobias.polley/Zotero/storage/2I2RWGCB/548351.html}
}

@article{uniprotconsortiumUniProtUniversalProtein2021,
  title = {{{UniProt}}: The Universal Protein Knowledgebase in 2021},
  shorttitle = {{{UniProt}}},
  author = {{UniProt Consortium}},
  year = {2021},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {49},
  number = {D1},
  pages = {D480-D489},
  issn = {1362-4962},
  doi = {10.1093/nar/gkaa1100},
  abstract = {The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this article, we describe significant updates that we have made over the last two years to the resource. The number of sequences in UniProtKB has risen to approximately 190 million, despite continued work to reduce sequence redundancy at the proteome level. We have adopted new methods of assessing proteome completeness and quality. We continue to extract detailed annotations from the literature to add to reviewed entries and supplement these in unreviewed entries with annotations provided by automated systems such as the newly implemented Association-Rule-Based Annotator (ARBA). We have developed a credit-based publication submission interface to allow the community to contribute publications and annotations to UniProt entries. We describe how UniProtKB responded to the COVID-19 pandemic through expert curation of relevant entries that were rapidly made available to the research community through a dedicated portal. UniProt resources are available under a CC-BY (4.0) license via the web at https://www.uniprot.org/.},
  langid = {english},
  pmcid = {PMC7778908},
  pmid = {33237286},
  keywords = {Computational Biology,COVID-19,Data Curation,Databases Protein,Humans,Internet,Knowledge Bases,Molecular Sequence Annotation,Pandemics,Proteome,Proteomics,SARS-CoV-2,User-Computer Interface,Viral Proteins},
  file = {/Users/tobias.polley/Zotero/storage/U7EEGKND/UniProt Consortium - 2021 - UniProt the universal protein knowledgebase in 20.pdf}
}

@article{veselovskyApproachVisualizationActive2001,
  title = {An {{Approach}} for {{Visualization}} of the {{Active Site}} of {{Enzymes}} with {{Unknown Three-Dimensional Structures}}},
  author = {Veselovsky, A. and Tikhonova, O. and Skvortsov, Vladlen S. and Medvedev, A. and Ivanov, A. S.},
  year = {2001},
  journal = {SAR and QSAR in Environmental Research},
  volume = {12},
  pages = {345--358},
  doi = {10.1080/10629360108033243},
  urldate = {2024-05-30},
  abstract = {Abstract A new approach for virtual characterization of the active site structure of enzymes with unknown three-dimensional (3D) structure has been proposed. It includes analysis of data on enzyme interaction with reversible competitive inhibitors, their 3D structures and moulding of the substrate-binding region. The superposition of ligands in biologically active conformations allows to determine the shape and dimension of the active site cavity accommodating these compounds. Monoamine oxidase A (MAO-A), a ``typical'' enzyme with unknown spatial organisation, was used to test this method. The correctness of such approach was validated by the analysis of HIV protease interaction with its inhibitors using 3D structures of their complexes. Mould of the substrate/inhibitor binding site can be used for the visualization of this binding site and for searching new ligands in molecular databases.},
  file = {/Users/tobias.polley/Zotero/storage/CUSMC9WJ/b12c36ed3fa05b34aff7f6c416fd1730.html}
}

@article{watanabeEnzymeNetResidualNeural2023,
  title = {{{EnzymeNet}}: Residual Neural Networks Model for {{Enzyme Commission}} Number Prediction},
  shorttitle = {{{EnzymeNet}}},
  author = {Watanabe, Naoki and Yamamoto, Masaki and Murata, Masahiro and Kuriya, Yuki and Araki, Michihiro},
  year = {2023},
  month = jan,
  journal = {Bioinformatics Advances},
  volume = {3},
  number = {1},
  pages = {vbad173},
  issn = {2635-0041},
  doi = {10.1093/bioadv/vbad173},
  urldate = {2024-06-25},
  abstract = {Enzymes are key targets to biosynthesize functional substances in metabolic engineering. Therefore, various machine learning models have been developed to predict Enzyme Commission (EC) numbers, one of the enzyme annotations. However, the previously reported models might predict the sequences with numerous consecutive identical amino acids, which are found within unannotated sequences, as enzymes.Here, we propose EnzymeNet for prediction of complete EC numbers using residual neural networks. EnzymeNet can exclude the exceptional sequences described above. Several EnzymeNet models were built and optimized to explore the best conditions for removing such sequences. As a result, the models exhibited higher prediction accuracy with macro F1 score up to 0.850 than previously reported models. Moreover, even the enzyme sequences with low similarity to training data, which were difficult to predict using the reported models, could be predicted extensively using EnzymeNet models. The robustness of EnzymeNet models will lead to discover novel enzymes for biosynthesis of functional compounds using microorganisms.The source code of EnzymeNet models is freely available at https://github.com/nwatanbe/enzymenet.},
  file = {/Users/tobias.polley/Zotero/storage/8JMESNQ4/Watanabe et al. - 2023 - EnzymeNet residual neural networks model for Enzy.pdf;/Users/tobias.polley/Zotero/storage/K2CFHFLL/7450150.html}
}

@article{wuUnifiedDeepLearning2018,
  title = {Unified {{Deep Learning Architecture}} for {{Modeling Biology Sequence}}},
  author = {Wu, Hongjie and Cao, Chengyuan and Xia, Xiaoyan and L{\"u}, Qiang},
  year = {2018},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {15},
  pages = {1445--1452},
  doi = {10.1109/TCBB.2017.2760832},
  urldate = {2024-06-12},
  abstract = {Prediction of the spatial structure or function of biological macromolecules based on their sequences remains an important challenge in bioinformatics. When modeling biological sequences using traditional sequencing models, long-range interaction, complicated and variable output of labeled structures, and variable length of biological sequences usually lead to different solutions on a case-by-case basis. This study proposed a unified deep learning architecture based on long short-term memory or a gated recurrent unit to capture long-range interactions. The architecture designs the optional reshape operator to adapt to the diversity of the output labels and implements a training algorithm to support the training of sequence models capable of processing variable-length sequences. The merging and pooling operators enhances the ability of capturing short-range interactions between basic units of biological sequences. The proposed deep-learning architecture and its training algorithm might be capable of solving currently variable biological sequence-modeling problems under a unified framework. We validated the model on one of the most difficult biological sequence-modeling problems, protein residue interaction prediction. The results indicate that the accuracy of obtaining the residue interactions of the model exceeded popular approaches by 10 percent on multiple widely-used benchmarks.},
  file = {/Users/tobias.polley/Zotero/storage/JPTVZWZ3/7efae8a7a0005feabfa297bc9e99f558.html}
}

@article{zhangEncoderdecoderModelsSequencetosequence2023,
  title = {Encoder-Decoder Models in Sequence-to-Sequence Learning: {{A}} Survey of {{RNN}} and {{LSTM}} Approaches},
  shorttitle = {Encoder-Decoder Models in Sequence-to-Sequence Learning},
  author = {Zhang, Yunong},
  year = {2023},
  journal = {Applied and Computational Engineering},
  doi = {10.54254/2755-2721/22/20231220},
  urldate = {2024-06-23},
  abstract = {In today's information age, from natural language processing to audio signal processing, from time series analysis to machine translation, the application of sequence data involves various fields. Encoder-decoder models, as a powerful approach to sequence modeling, have attracted extensive attention and research. This review paper aims to explore encoder-decoder models, focusing on the principles and operational steps of recurrent neural networks (RNN) and long short-term memory (LSTM), aiming to provide researchers and practitioners with a deep understanding of the fundamentals and applications of these models Condition. Through the analysis and summary of relevant literature, this study reveals the advantages of RNN and LSTM in sequence data processing; RNN structure is simple and effective, can process sequence input and output of any length, and can capture the time dependence in sequence data. But there is a problem of gradient disappearance, which makes it difficult to deal with long-term dependencies. To solve this problem, the LSTM model introduces different gating mechanisms, which effectively solve the gradient problem while better capturing long-term dependencies. Through the review of the principles and applications of the two, it provides a useful reference for further research and application.},
  file = {/Users/tobias.polley/Zotero/storage/JBNT3XXH/Zhang - 2023 - Encoder-decoder models in sequence-to-sequence lea.pdf;/Users/tobias.polley/Zotero/storage/MJFBB8JR/3998238d4cd45707bb7bf2ca98cac6f5.html}
}
